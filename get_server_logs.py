# Copyright 2012 Viewfinder Inc. All Rights Reserved.

"""Merge server logs from S3 into daily directories, with one file per instance.

Does the following:
1) fetch list of log files from S3://serverlog-viewfinder-co/viewfinder/full/
   fetch list of processed files from file S3://serverlog-viewfinder-co/processed_logs/viewfinder/full/PROCESSED
2) compute list of raw log files that need to be processed
3) iterate over all log files and write filtered entries (by module) to a per-day/per-instance file
   if a file exists in S3 for that day/instance, fetch it first to merge
4) when all files have been processed, upload all working files to S3 and update the list of processed files.

The resulting merged files are in: S3://serverlog-viewfinder-co/processed_logs/viewfinder/full/YYYY-MM-DD/<instance>
All entries in a given merged file are guaranteed to be for that date only. A single file is created per instance.

Only log entries generated by one of the following modules are kept:
['identity', 'operation', 'service', 'user_op_manager', 'web']

When processing a raw log file, entries are not directly written to the new merged file(s) but instead kept in a
per-merged-file buffer which is only written our once the raw log has been fully processed. At that point, the
filename of the raw log file is also added to the "processed" list.

When all log files to be processed have been, upload all files to S3 and update the registry with the list of
files successfilly processed.

Usage:
# Process all new server logs since last fetch.
python -m viewfinder.backend.logs.get_server_logs

# Search for new logs starting on December 1st 2012 (S3 ListKeys prefix).
python -m viewfinder.backend.logs.get_server_logs --start_date=2012-12-01

Other flags:
-dry_run: default=True: do everything, but do not write processed logs files to S3 or update registry.
-ec2_only: default=True: only analyze logs from AWS instances.
-require_lock: default=True: hold the job:merge_logs lock during processing.
-max_files_to_process: default=None: process at most this many raw log files.

"""

__author__ = 'marc@emailscrubbed.com (Marc Berhault)'

import cStringIO
import logging
import os
import re
import sys

from tornado import gen, options
from viewfinder.backend.base import main, retry
from viewfinder.backend.db import db_client
from viewfinder.backend.db.job import Job
from viewfinder.backend.logs import logs_util, log_merger
from viewfinder.backend.storage import store_utils
from viewfinder.backend.storage.object_store import ObjectStore

options.define('start_date', default=None, help='Start date (filename start key)')
options.define('dry_run', default=True, help='Do not write processed logs to S3 or update registry')
options.define('ec2_only', default=True, help='AWS instances only')
options.define('require_lock', type=bool, default=True,
               help='attempt to grab the job:merge_logs lock before running. Exit if acquire fails.')
options.define('max_files_to_process', type=int, default=None, help='Maximum number of files to process.')

# Retry policy for uploading files to S3 (merge logs and registry).
kS3UploadRetryPolicy = retry.RetryPolicy(max_tries=5, timeout=300,
                                         min_delay=1, max_delay=30,
                                         check_exception=retry.RetryPolicy.AlwaysRetryOnException)

@gen.engine
def ProcessFiles(logs_store, merged_store, logs_paths, filenames, dry_run, callback):
  """Process each file contained in 'filenames'. Returns a list of successfully processed file names."""
  processed_files = []
  # Dict of (instance, date) -> LocalLogMerge
  day_instance_logs = {}
  s3_base = logs_paths.MergedDirectory()

  @gen.engine
  def _AddEntry(instance, entry, callback):
    """Given a single log entry, validate it, find (or create) the corresponding LocalLogMerge and add it."""
    if not entry:
      callback()
      return
    parsed = logs_util.ParseLogLine(entry)
    if not parsed:
      callback()
      return
    day, _, _, _ = parsed

    # We can't use setdefault here, as it would create a new LocalLogMerge instance.
    day_log = None
    if (instance, day) not in day_instance_logs:
      # S3 filenames will be: <object_store>/s3_base/day/instance
      day_instance_logs[(instance, day)] = log_merger.LocalLogMerge(merged_store, [day, instance], s3_base)
      day_log = day_instance_logs[(instance, day)]
      yield gen.Task(day_log.FetchExistingFromS3)
    day_log = day_instance_logs[(instance, day)]
    day_log.Append(entry)
    callback()

  @gen.engine
  def _ProcessOneFile(instance, contents, callback):
    """Iterate over a raw log file. Any line that does not start with a date in the form YYYY-MM-DD is considered
    part of a multi-line entry.
    TODO(marc): this will break horribly if we ever have multiple processes/threads logging to the same file.
    """
    buf = cStringIO.StringIO(contents)
    buf.seek(0)
    entry = ''
    while True:
      line = buf.readline()
      if not line:
        yield gen.Task(_AddEntry, instance, entry)
        break
      if line.startswith((' ', '\t')) or logs_util.ParseLogLine(line) is None:
        # This is a followup to a multi-line entry.
        entry += ' ' + line.strip()
      else:
        # New entry: write the previous one.
        yield gen.Task(_AddEntry, instance, entry)
        entry = line.strip()
    buf.close()
    callback()

  for filename in filenames:
    instance = logs_paths.RawLogPathToInstance(filename)
    assert instance
    contents = ''
    try:
      contents = yield gen.Task(logs_store.Get, filename)
    except Exception as e:
      logging.error('Error fetching file %s: %r' % (filename, e))
      continue

    logging.info('processing %d bytes from raw log %s' % (len(contents), filename))
    yield gen.Task(_ProcessOneFile, instance, contents)
    processed_files.append(filename)
    # We need to flush all DayInstance logs as we've marked the file as successfully processed.
    for log in day_instance_logs.values():
      log.FlushBuffer()

  # Close all LocalMergeLog objects and upload to S3.
  for instance_day, log in day_instance_logs.iteritems():
    log.Close()
    if not dry_run:
      try:
        yield gen.Task(log.Upload)
      except Exception as e:
        # Errors in the Put are unrecoverable. We can't upload the registry after a failed merged log upload.
        # TODO(marc): provide a way to mark a given day's merged logs as bad and force recompute.
        logging.error('Error uploading file to S3 for %r: %r' % (instance_day, e))
    log.Cleanup()
  callback(processed_files)


@gen.engine
def GetRawLogsFileList(logs_store, logs_paths, marker, callback):
  """Fetch the list of file names from S3."""
  def _WantFile(filename):
    instance = logs_paths.RawLogPathToInstance(filename)
    if instance is None:
      logging.error('Could not extract instance from file name %s' % filename)
      return False
    return not options.options.ec2_only or logs_util.IsEC2Instance(instance)

  base_path = logs_paths.RawDirectory()
  marker = os.path.join(base_path, marker) if marker is not None else None
  file_list = yield gen.Task(store_utils.ListAllKeys, logs_store, prefix=base_path, marker=marker)
  files = [f for f in file_list if _WantFile(f)]

  logging.info('found %d total raw log files, %d important ones' % (len(file_list), len(files)))
  callback(files)


@gen.engine
def RunOnce(callback):
  """Get list of files and call processing function."""
  dry_run = options.options.dry_run

  logs_paths = logs_util.ServerLogsPaths('viewfinder', 'full')

  if dry_run:
    logging.warning('dry_run=True: will not upload processed logs files or update registry')

  logs_store = ObjectStore.GetInstance(logs_paths.SOURCE_LOGS_BUCKET)
  merged_store = ObjectStore.GetInstance(logs_paths.MERGED_LOGS_BUCKET)

  # Fetch list of raw logs files.
  files = yield gen.Task(GetRawLogsFileList, logs_store, logs_paths, options.options.start_date)

  # Fetch list of processed logs files from registry.
  processed_files = yield gen.Task(logs_util.GetRegistry, merged_store, logs_paths.ProcessedRegistryPath())
  if processed_files is None:
    # None means: registry does not exist. All other errors throw exceptions.
    processed_files = []

  # Compute list of raw files to process (and sort by filename -> sort by date).
  files_set = set(files)
  processed_set = set(processed_files)
  missing_files = list(files_set.difference(processed_set))
  missing_files.sort()

  to_process = missing_files
  if options.options.max_files_to_process is not None:
    to_process = missing_files[0:options.options.max_files_to_process]

  logging.info('found %d raw files and %d processed files, %d missing. Will process %d.' %
               (len(files), len(processed_files), len(missing_files), len(to_process)))
  if len(missing_files) == 0:
    logging.info('No raw logs files to process.')
    callback()
    return

  merged_files = yield gen.Task(ProcessFiles, logs_store, merged_store, logs_paths, to_process, dry_run)
  logging.info('found %d raw files and %d processed files, %d missing, successfully processed %d' %
               (len(files), len(processed_files), len(missing_files), len(merged_files)))

  # Add processed files to registry and write to S3.
  # TODO(marc): any failure in merged log upload or registry upload will cause us to get out of sync. To fix this,
  # we should also have a list of properly applied processed logs.
  processed_files.extend(merged_files)
  processed_files.sort()
  if not dry_run:
    yield gen.Task(retry.CallWithRetryAsync, kS3UploadRetryPolicy,
                   logs_util.WriteRegistry, merged_store, logs_paths.ProcessedRegistryPath(), processed_files)

  callback()

@gen.engine
def _Start(callback):
  """Grab the job lock and call RunOnce if acquired. We do not write a job summary as we currently do not use it."""
  client = db_client.DBClient.Instance()
  job = Job(client, 'merge_logs')

  if options.options.require_lock:
    got_lock = yield gen.Task(job.AcquireLock)
    if got_lock == False:
      logging.warning('Failed to acquire job lock: exiting.')
      callback()
      return

  try:
    yield gen.Task(RunOnce)
  finally:
    yield gen.Task(job.ReleaseLock)

  callback()


if __name__ == '__main__':
  sys.exit(main.InitAndRun(_Start))
